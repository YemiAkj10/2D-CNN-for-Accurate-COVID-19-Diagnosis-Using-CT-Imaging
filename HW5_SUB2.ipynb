{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44d9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "from torch.utils.data import DataLoader as torch_dataloader\n",
    "from torch.utils.data import Dataset as torch_dataset\n",
    "import torch.optim as optim\n",
    "#from data_loader import get_dataloader\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import skimage.io as io\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f027b9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yemiakj10/Desktop/MSDS 2/Machine Learning/Main Story/GradCam/val.csv\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the relative path to the text file or data file\n",
    "file_name = 'val.csv'  # Replace 'your_text_file.txt' with the actual file name\n",
    "\n",
    "# Construct the absolute file path by joining the current directory and the file name\n",
    "absolute_file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "print(absolute_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59da3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RandomResNet18, self).__init__()\n",
    "        # Load the pre-trained ResNet-18 model\n",
    "        self.resnet = models.resnet18(pretrained=False)\n",
    "     \n",
    "       # Modify the final fully connected layer for binary classification\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the modified ResNet-18 model\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ffc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(PretrainedResNet18, self).__init__()\n",
    "        # Load the pre-trained ResNet-18 model\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove the original fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Add a new fully connected layer for binary classification\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the modified ResNet-18 model\n",
    "        x = self.features(x)\n",
    "        # Determine the size of the feature maps\n",
    "        feature_size = x.size(1)\n",
    "        # Flatten the feature maps\n",
    "        x = x.view(x.size(0), feature_size)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9f74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, model, optimizer, result, epoch):\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'result':result},\n",
    "               filename)\n",
    "    print('saved:', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6797810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch_dataset):\n",
    "    def __init__(self, path, filenamelist, labellist):\n",
    "        self.path=path\n",
    "        self.filenamelist=filenamelist\n",
    "        self.labellist=labellist\n",
    "    def __len__(self):\n",
    "        #return the number of data points\n",
    "        return len(self.filenamelist)\n",
    "    def __getitem__(self, idx):\n",
    "        I=io.imread(self.path+self.filenamelist[idx])\n",
    "        I=skimage.util.img_as_float32(I)\n",
    "        I = I.reshape(1,I.shape[0],I.shape[1])\n",
    "        I = torch.tensor(I, dtype=torch.float32)\n",
    "        I = I.expand(3, I.shape[1],I.shape[2])\n",
    "        label=torch.tensor(self.labellist[idx], dtype=torch.int64)\n",
    "        return I, label\n",
    "#%%\n",
    "def get_dataloader(path='/Users/yemiakj10/Desktop/MSDS 2/Machine Learning/Main Story/GradCam/'):\n",
    "    df_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "    df_val = pd.read_csv(os.path.join(path, 'val.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "    dataset_train = MyDataset(path, df_train['filename'].values, df_train['label'].values)\n",
    "    dataset_val = MyDataset(path, df_val['filename'].values, df_val['label'].values)\n",
    "    dataset_test = MyDataset(path, df_test['filename'].values, df_test['label'].values)\n",
    "    loader_train = torch_dataloader(dataset_train, batch_size=32, num_workers=0,\n",
    "                                    shuffle=True, pin_memory=True)\n",
    "    loader_val = torch_dataloader(dataset_val, batch_size=32, num_workers=0,\n",
    "                                  shuffle=False, pin_memory=True)\n",
    "    loader_test = torch_dataloader(dataset_test, batch_size=32, num_workers=0,\n",
    "                                   shuffle=False, pin_memory=True)\n",
    "\n",
    "\t#modify this function to return loader_train, loader_val, and loader_test\n",
    "    return loader_train, loader_val, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c071a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(confusion):\n",
    "    #input: confusion is the confusion matrix\n",
    "    #output: acc is the standard classification accuracy\n",
    "    M=confusion.copy().astype('float32')\n",
    "    acc = M.diagonal().sum()/M.sum()    \n",
    "    sens=np.zeros(M.shape[0])\n",
    "    prec=np.zeros(M.shape[0]) \n",
    "    for n in range(0, M.shape[0]):\n",
    "        TP=M[n,n]\n",
    "        FN=np.sum(M[n,:])-TP\n",
    "        FP=np.sum(M[:,n])-TP\n",
    "        sens[n]=TP/(TP+FN)\n",
    "        prec[n]=TP/(TP+FP)       \n",
    "    return acc, sens, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf4417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, dataloader, epoch):    \n",
    "    model.train()#set model to training mode\n",
    "    loss_train=0\n",
    "    acc_train =0 \n",
    "    sample_count=0\n",
    "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()#clear grad of each parameter\n",
    "        Z = model(X)#forward pass\n",
    "        loss = nnF.cross_entropy(Z, Y)\n",
    "        loss.backward()#backward pass\n",
    "        optimizer.step()#update parameters\n",
    "        loss_train+=loss.item()\n",
    "        #do not need softmax\n",
    "        Yp = Z.data.max(dim=1)[1]  # get the index of the max               \n",
    "        acc_train+= torch.sum(Yp==Y).item()\n",
    "        sample_count+=X.size(0)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, 100. * batch_idx / len(dataloader), loss.item()))\n",
    "    loss_train/=len(dataloader)\n",
    "    #due to upsampling, len(dataloader.dataset) != sample_count\n",
    "    #acc_train/=len(dataloader.dataset) \n",
    "    acc_train/=sample_count    \n",
    "    return loss_train, acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc8658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, dataloader):\n",
    "    model.eval()#set model to evaluation mode\n",
    "    acc_test =0\n",
    "    confusion=np.zeros((5,5))\n",
    "    with torch.no_grad(): # tell Pytorch not to build graph in the with section\n",
    "        for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            Z = model(X)#forward pass\n",
    "            #do not need softmax\n",
    "            Yp = Z.data.max(dim=1)[1]  # get the index of the max \n",
    "            acc_test+= torch.sum(Yp==Y).item()\n",
    "            for i in range(0, 5):\n",
    "                for j in range(0, 5):\n",
    "                    confusion[i,j]+=torch.sum((Y==i)&(Yp==j)).item()\n",
    "    acc, sens, prec=cal_accuracy(confusion)\n",
    "    return acc, (confusion, sens, prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab447066",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fc3c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yemiakj10/Anaconda/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yemiakj10/Anaconda/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_Ran = RandomResNet18()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(model_Ran.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91718104",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train, loader_val, loader_test = get_dataloader()\n",
    "loss_train_list=[]\n",
    "acc_train_list=[]\n",
    "acc_val_list=[]\n",
    "epoch_save=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b741abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0%]\tLoss: 0.717220\n",
      "epoch 0 training loss: 0.4377221640897915 acc: 0.8115727002967359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sens[n]=TP/(TP+FN)\n",
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec[n]=TP/(TP+FP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation acc: 0.78333336\n",
      "saved: Covid_19_CT_Images_Random0.pt\n",
      "Train Epoch: 1 [0%]\tLoss: 0.313438\n",
      "epoch 1 training loss: 0.2304337655659765 acc: 0.9070227497527201\n",
      "epoch 1 validation acc: 0.8333333\n",
      "saved: Covid_19_CT_Images_Random1.pt\n",
      "Train Epoch: 2 [0%]\tLoss: 0.167972\n",
      "epoch 2 training loss: 0.1785069305333309 acc: 0.9213649851632048\n",
      "epoch 2 validation acc: 0.53333336\n",
      "saved: Covid_19_CT_Images_Random2.pt\n",
      "Train Epoch: 3 [0%]\tLoss: 0.072565\n",
      "epoch 3 training loss: 0.1279698265134357 acc: 0.9515331355093967\n",
      "epoch 3 validation acc: 0.6\n",
      "saved: Covid_19_CT_Images_Random3.pt\n",
      "Train Epoch: 4 [0%]\tLoss: 0.051702\n",
      "epoch 4 training loss: 0.07191089003663365 acc: 0.9723046488625123\n",
      "epoch 4 validation acc: 0.9\n",
      "saved: Covid_19_CT_Images_Random4.pt\n",
      "Train Epoch: 5 [0%]\tLoss: 0.010340\n",
      "epoch 5 training loss: 0.03170907261301181 acc: 0.9886251236399605\n",
      "epoch 5 validation acc: 0.85\n",
      "saved: Covid_19_CT_Images_Random5.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_save+1, 6): #change 100 to a larger number if necessary\n",
    "    #-------- training --------------------------------\n",
    "    loss_train, acc_train =train(model_Ran, device, optimizer, loader_train, epoch)    \n",
    "    loss_train_list.append(loss_train)\n",
    "    acc_train_list.append(acc_train)\n",
    "    print('epoch', epoch, 'training loss:', loss_train, 'acc:', acc_train)\n",
    "    #-------- validation --------------------------------\n",
    "    acc_val, other_val = test(model_Ran, device, loader_val)\n",
    "    acc_val_list.append(acc_val)\n",
    "    print('epoch', epoch, 'validation acc:', acc_val)\n",
    "    #-------- test --------------------------------------\n",
    "    #acc_test, other_test = test(model_Ran, device, loader_test)\n",
    "    #acc_test_list.append(acc_test)\n",
    "    #print('epoch', epoch, 'test acc:', acc_test)\n",
    "    #--------save model-------------------------\n",
    "    result = (loss_train_list, acc_train_list, \n",
    "              acc_val_list, other_val)\n",
    "    save_checkpoint('Covid_19_CT_Images_Random'+str(epoch)+'.pt', model_Ran, optimizer, result, epoch)\n",
    "    epoch_save=epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b62ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sens[n]=TP/(TP+FN)\n",
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec[n]=TP/(TP+FP)\n"
     ]
    }
   ],
   "source": [
    "acc_test, other_test = test(model_Ran, device, loader_test)\n",
    "print('Test acc:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba09044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yemiakj10/Anaconda/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yemiakj10/Anaconda/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_Pre = PretrainedResNet18()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(model_Pre.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8003c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train, loader_val, loader_test = get_dataloader()\n",
    "\n",
    "loss_train_list=[]\n",
    "acc_train_list=[]\n",
    "acc_val_list=[]\n",
    "epoch_save=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac82af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0%]\tLoss: 0.763352\n",
      "epoch 0 training loss: 0.29221851969487034 acc: 0.8882294757665677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sens[n]=TP/(TP+FN)\n",
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec[n]=TP/(TP+FP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation acc: 0.76666665\n",
      "saved: Covid_19_CT_Images_Pretrained0.pt\n",
      "Train Epoch: 1 [0%]\tLoss: 0.056118\n",
      "epoch 1 training loss: 0.05128537997370586 acc: 0.983679525222552\n",
      "epoch 1 validation acc: 0.96666664\n",
      "saved: Covid_19_CT_Images_Pretrained1.pt\n",
      "Train Epoch: 2 [0%]\tLoss: 0.004243\n",
      "epoch 2 training loss: 0.03969997144758963 acc: 0.9846686449060337\n",
      "epoch 2 validation acc: 0.96666664\n",
      "saved: Covid_19_CT_Images_Pretrained2.pt\n",
      "Train Epoch: 3 [0%]\tLoss: 0.036026\n",
      "epoch 3 training loss: 0.022294436599622713 acc: 0.9930761622156281\n",
      "epoch 3 validation acc: 0.9166667\n",
      "saved: Covid_19_CT_Images_Pretrained3.pt\n",
      "Train Epoch: 4 [0%]\tLoss: 0.001679\n",
      "epoch 4 training loss: 0.015381679459551378 acc: 0.9960435212660732\n",
      "epoch 4 validation acc: 0.8333333\n",
      "saved: Covid_19_CT_Images_Pretrained4.pt\n",
      "Train Epoch: 5 [0%]\tLoss: 0.002155\n",
      "epoch 5 training loss: 0.00672489255543951 acc: 0.9990108803165183\n",
      "epoch 5 validation acc: 0.95\n",
      "saved: Covid_19_CT_Images_Pretrained5.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_save+1, 6): #change 100 to a larger number if necessary\n",
    "    #-------- training --------------------------------\n",
    "    loss_train, acc_train =train(model_Pre, device, optimizer, loader_train, epoch)    \n",
    "    loss_train_list.append(loss_train)\n",
    "    acc_train_list.append(acc_train)\n",
    "    print('epoch', epoch, 'training loss:', loss_train, 'acc:', acc_train)\n",
    "    #-------- validation --------------------------------\n",
    "    acc_val, other_val = test(model_Pre, device, loader_val)\n",
    "    acc_val_list.append(acc_val)\n",
    "    print('epoch', epoch, 'validation acc:', acc_val)\n",
    "    #-------- test --------------------------------------\n",
    "    #acc_test, other_test = test(model_Pre, device, loader_test)\n",
    "    #acc_test_list.append(acc_test)\n",
    "    #print('epoch', epoch, 'test acc:', acc_test)\n",
    "    #--------save model-------------------------\n",
    "    result = (loss_train_list, acc_train_list, \n",
    "              acc_val_list, other_val)\n",
    "    save_checkpoint('Covid_19_CT_Images_Pretrained'+str(epoch)+'.pt', model_Pre, optimizer, result, epoch)\n",
    "    epoch_save=epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a7d262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sens[n]=TP/(TP+FN)\n",
      "/var/folders/1q/x916rngd0bl3jwtyp7zz6h1w0000gn/T/ipykernel_830/3076453557.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec[n]=TP/(TP+FP)\n"
     ]
    }
   ],
   "source": [
    "acc_test, other_test = test(model_Pre, device, loader_test)\n",
    "print('Test acc:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfcac2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "resnet.conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "resnet.bn1.weight \t torch.Size([64])\n",
      "resnet.bn1.bias \t torch.Size([64])\n",
      "resnet.bn1.running_mean \t torch.Size([64])\n",
      "resnet.bn1.running_var \t torch.Size([64])\n",
      "resnet.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn1.weight \t torch.Size([64])\n",
      "resnet.layer1.0.bn1.bias \t torch.Size([64])\n",
      "resnet.layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "resnet.layer1.0.bn1.running_var \t torch.Size([64])\n",
      "resnet.layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.0.bn2.weight \t torch.Size([64])\n",
      "resnet.layer1.0.bn2.bias \t torch.Size([64])\n",
      "resnet.layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "resnet.layer1.0.bn2.running_var \t torch.Size([64])\n",
      "resnet.layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn1.weight \t torch.Size([64])\n",
      "resnet.layer1.1.bn1.bias \t torch.Size([64])\n",
      "resnet.layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "resnet.layer1.1.bn1.running_var \t torch.Size([64])\n",
      "resnet.layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "resnet.layer1.1.bn2.weight \t torch.Size([64])\n",
      "resnet.layer1.1.bn2.bias \t torch.Size([64])\n",
      "resnet.layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "resnet.layer1.1.bn2.running_var \t torch.Size([64])\n",
      "resnet.layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "resnet.layer2.0.bn1.weight \t torch.Size([128])\n",
      "resnet.layer2.0.bn1.bias \t torch.Size([128])\n",
      "resnet.layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "resnet.layer2.0.bn1.running_var \t torch.Size([128])\n",
      "resnet.layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.0.bn2.weight \t torch.Size([128])\n",
      "resnet.layer2.0.bn2.bias \t torch.Size([128])\n",
      "resnet.layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "resnet.layer2.0.bn2.running_var \t torch.Size([128])\n",
      "resnet.layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "resnet.layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "resnet.layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "resnet.layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "resnet.layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "resnet.layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn1.weight \t torch.Size([128])\n",
      "resnet.layer2.1.bn1.bias \t torch.Size([128])\n",
      "resnet.layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "resnet.layer2.1.bn1.running_var \t torch.Size([128])\n",
      "resnet.layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "resnet.layer2.1.bn2.weight \t torch.Size([128])\n",
      "resnet.layer2.1.bn2.bias \t torch.Size([128])\n",
      "resnet.layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "resnet.layer2.1.bn2.running_var \t torch.Size([128])\n",
      "resnet.layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "resnet.layer3.0.bn1.weight \t torch.Size([256])\n",
      "resnet.layer3.0.bn1.bias \t torch.Size([256])\n",
      "resnet.layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "resnet.layer3.0.bn1.running_var \t torch.Size([256])\n",
      "resnet.layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.0.bn2.weight \t torch.Size([256])\n",
      "resnet.layer3.0.bn2.bias \t torch.Size([256])\n",
      "resnet.layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "resnet.layer3.0.bn2.running_var \t torch.Size([256])\n",
      "resnet.layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "resnet.layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "resnet.layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "resnet.layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "resnet.layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "resnet.layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn1.weight \t torch.Size([256])\n",
      "resnet.layer3.1.bn1.bias \t torch.Size([256])\n",
      "resnet.layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "resnet.layer3.1.bn1.running_var \t torch.Size([256])\n",
      "resnet.layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "resnet.layer3.1.bn2.weight \t torch.Size([256])\n",
      "resnet.layer3.1.bn2.bias \t torch.Size([256])\n",
      "resnet.layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "resnet.layer3.1.bn2.running_var \t torch.Size([256])\n",
      "resnet.layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "resnet.layer4.0.bn1.weight \t torch.Size([512])\n",
      "resnet.layer4.0.bn1.bias \t torch.Size([512])\n",
      "resnet.layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "resnet.layer4.0.bn1.running_var \t torch.Size([512])\n",
      "resnet.layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.0.bn2.weight \t torch.Size([512])\n",
      "resnet.layer4.0.bn2.bias \t torch.Size([512])\n",
      "resnet.layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "resnet.layer4.0.bn2.running_var \t torch.Size([512])\n",
      "resnet.layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "resnet.layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "resnet.layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "resnet.layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "resnet.layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "resnet.layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn1.weight \t torch.Size([512])\n",
      "resnet.layer4.1.bn1.bias \t torch.Size([512])\n",
      "resnet.layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "resnet.layer4.1.bn1.running_var \t torch.Size([512])\n",
      "resnet.layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "resnet.layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "resnet.layer4.1.bn2.weight \t torch.Size([512])\n",
      "resnet.layer4.1.bn2.bias \t torch.Size([512])\n",
      "resnet.layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "resnet.layer4.1.bn2.running_var \t torch.Size([512])\n",
      "resnet.layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "resnet.fc.weight \t torch.Size([2, 512])\n",
      "resnet.fc.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model_Ran.state_dict():\n",
    "    print(param_tensor, \"\\t\", model_Ran.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca155421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "features.0.weight \t torch.Size([64, 3, 7, 7])\n",
      "features.1.weight \t torch.Size([64])\n",
      "features.1.bias \t torch.Size([64])\n",
      "features.1.running_mean \t torch.Size([64])\n",
      "features.1.running_var \t torch.Size([64])\n",
      "features.1.num_batches_tracked \t torch.Size([])\n",
      "features.4.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "features.4.0.bn1.weight \t torch.Size([64])\n",
      "features.4.0.bn1.bias \t torch.Size([64])\n",
      "features.4.0.bn1.running_mean \t torch.Size([64])\n",
      "features.4.0.bn1.running_var \t torch.Size([64])\n",
      "features.4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.4.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "features.4.0.bn2.weight \t torch.Size([64])\n",
      "features.4.0.bn2.bias \t torch.Size([64])\n",
      "features.4.0.bn2.running_mean \t torch.Size([64])\n",
      "features.4.0.bn2.running_var \t torch.Size([64])\n",
      "features.4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.4.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "features.4.1.bn1.weight \t torch.Size([64])\n",
      "features.4.1.bn1.bias \t torch.Size([64])\n",
      "features.4.1.bn1.running_mean \t torch.Size([64])\n",
      "features.4.1.bn1.running_var \t torch.Size([64])\n",
      "features.4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.4.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "features.4.1.bn2.weight \t torch.Size([64])\n",
      "features.4.1.bn2.bias \t torch.Size([64])\n",
      "features.4.1.bn2.running_mean \t torch.Size([64])\n",
      "features.4.1.bn2.running_var \t torch.Size([64])\n",
      "features.4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.5.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "features.5.0.bn1.weight \t torch.Size([128])\n",
      "features.5.0.bn1.bias \t torch.Size([128])\n",
      "features.5.0.bn1.running_mean \t torch.Size([128])\n",
      "features.5.0.bn1.running_var \t torch.Size([128])\n",
      "features.5.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.5.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "features.5.0.bn2.weight \t torch.Size([128])\n",
      "features.5.0.bn2.bias \t torch.Size([128])\n",
      "features.5.0.bn2.running_mean \t torch.Size([128])\n",
      "features.5.0.bn2.running_var \t torch.Size([128])\n",
      "features.5.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.5.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "features.5.0.downsample.1.weight \t torch.Size([128])\n",
      "features.5.0.downsample.1.bias \t torch.Size([128])\n",
      "features.5.0.downsample.1.running_mean \t torch.Size([128])\n",
      "features.5.0.downsample.1.running_var \t torch.Size([128])\n",
      "features.5.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "features.5.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "features.5.1.bn1.weight \t torch.Size([128])\n",
      "features.5.1.bn1.bias \t torch.Size([128])\n",
      "features.5.1.bn1.running_mean \t torch.Size([128])\n",
      "features.5.1.bn1.running_var \t torch.Size([128])\n",
      "features.5.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.5.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "features.5.1.bn2.weight \t torch.Size([128])\n",
      "features.5.1.bn2.bias \t torch.Size([128])\n",
      "features.5.1.bn2.running_mean \t torch.Size([128])\n",
      "features.5.1.bn2.running_var \t torch.Size([128])\n",
      "features.5.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.6.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "features.6.0.bn1.weight \t torch.Size([256])\n",
      "features.6.0.bn1.bias \t torch.Size([256])\n",
      "features.6.0.bn1.running_mean \t torch.Size([256])\n",
      "features.6.0.bn1.running_var \t torch.Size([256])\n",
      "features.6.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.6.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.6.0.bn2.weight \t torch.Size([256])\n",
      "features.6.0.bn2.bias \t torch.Size([256])\n",
      "features.6.0.bn2.running_mean \t torch.Size([256])\n",
      "features.6.0.bn2.running_var \t torch.Size([256])\n",
      "features.6.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.6.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "features.6.0.downsample.1.weight \t torch.Size([256])\n",
      "features.6.0.downsample.1.bias \t torch.Size([256])\n",
      "features.6.0.downsample.1.running_mean \t torch.Size([256])\n",
      "features.6.0.downsample.1.running_var \t torch.Size([256])\n",
      "features.6.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "features.6.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.6.1.bn1.weight \t torch.Size([256])\n",
      "features.6.1.bn1.bias \t torch.Size([256])\n",
      "features.6.1.bn1.running_mean \t torch.Size([256])\n",
      "features.6.1.bn1.running_var \t torch.Size([256])\n",
      "features.6.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.6.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.6.1.bn2.weight \t torch.Size([256])\n",
      "features.6.1.bn2.bias \t torch.Size([256])\n",
      "features.6.1.bn2.running_mean \t torch.Size([256])\n",
      "features.6.1.bn2.running_var \t torch.Size([256])\n",
      "features.6.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.7.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "features.7.0.bn1.weight \t torch.Size([512])\n",
      "features.7.0.bn1.bias \t torch.Size([512])\n",
      "features.7.0.bn1.running_mean \t torch.Size([512])\n",
      "features.7.0.bn1.running_var \t torch.Size([512])\n",
      "features.7.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.7.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.7.0.bn2.weight \t torch.Size([512])\n",
      "features.7.0.bn2.bias \t torch.Size([512])\n",
      "features.7.0.bn2.running_mean \t torch.Size([512])\n",
      "features.7.0.bn2.running_var \t torch.Size([512])\n",
      "features.7.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "features.7.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "features.7.0.downsample.1.weight \t torch.Size([512])\n",
      "features.7.0.downsample.1.bias \t torch.Size([512])\n",
      "features.7.0.downsample.1.running_mean \t torch.Size([512])\n",
      "features.7.0.downsample.1.running_var \t torch.Size([512])\n",
      "features.7.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "features.7.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.7.1.bn1.weight \t torch.Size([512])\n",
      "features.7.1.bn1.bias \t torch.Size([512])\n",
      "features.7.1.bn1.running_mean \t torch.Size([512])\n",
      "features.7.1.bn1.running_var \t torch.Size([512])\n",
      "features.7.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "features.7.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.7.1.bn2.weight \t torch.Size([512])\n",
      "features.7.1.bn2.bias \t torch.Size([512])\n",
      "features.7.1.bn2.running_mean \t torch.Size([512])\n",
      "features.7.1.bn2.running_var \t torch.Size([512])\n",
      "features.7.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([2, 512])\n",
      "fc.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model_Pre.state_dict():\n",
    "    print(param_tensor, \"\\t\", model_Pre.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d712c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the model\n",
    "directory = \"/Users/yemiakj10/Desktop/MSDS 2/Machine Learning/Main Story/GradCam\"\n",
    "\n",
    "# Specify the file name for the model\n",
    "filename = \"model_Ran.pth\"\n",
    "\n",
    "# Combine directory and file name to create the complete file path\n",
    "PATH = os.path.join(directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fdc0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_Ran.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2819212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the model\n",
    "directory = \"/Users/yemiakj10/Desktop/MSDS 2/Machine Learning/Main Story/GradCam\"\n",
    "\n",
    "# Specify the file name for the model\n",
    "filename = \"model_Ran.pth\"\n",
    "\n",
    "# Combine directory and file name to create the complete file path\n",
    "PATH = os.path.join(directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the model\n",
    "directory = \"/Users/yemiakj10/Desktop/MSDS 2/Machine Learning/Main Story/GradCam\"\n",
    "\n",
    "# Specify the file name for the model\n",
    "filename = \"model_Pre.pth\"\n",
    "\n",
    "# Combine directory and file name to create the complete file path\n",
    "PATH = os.path.join(directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c806778",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_Pre.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
